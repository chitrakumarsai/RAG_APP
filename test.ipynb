{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "\n",
    "documents = SimpleDirectoryReader(input_dir='LLM documents').load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7f6bae63-6550-42f6-a8c2-4042ef2c39ee'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 340/340 [00:00<00:00, 4468.22it/s]\n",
      "Generating embeddings: 100%|██████████| 340/340 [00:03<00:00, 107.13it/s]\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: A transformer is a sequence-to-sequence model that\n",
      "consists of an encoder and a decoder. The encoder converts input text\n",
      "into a representation, which is then passed to the decoder for\n",
      "generating output text. The transformer architecture was developed at\n",
      "Google in 2017 for tasks like translation, where it can convert\n",
      "sequences from one domain into sequences in another domain.\n",
      "______________________________________________________________________\n",
      "Source Node 1/2\n",
      "Node ID: 484185cd-7b42-469d-a346-e9f0edcc59f8\n",
      "Similarity: 0.8106627408217553\n",
      "Text: Foundational Large Language Models & Text Generation 9 September\n",
      "2024 Transformer The transformer architecture was developed at Google\n",
      "in 2017 for use in a translation model.1  It’s a sequence-to-sequence\n",
      "model capable of converting sequences from one domain  into sequences\n",
      "in another domain. For example, translating French sentences to\n",
      "English ...\n",
      "______________________________________________________________________\n",
      "Source Node 2/2\n",
      "Node ID: fb9209e1-2ce4-4d03-820d-4d76bfb3b9d2\n",
      "Similarity: 0.7876712263819428\n",
      "Text: Foundational Large Language Models & Text Generation 11\n",
      "September 2024 To better understand the different layers in the\n",
      "transformer, let’s use a French-to-English  translation task as an\n",
      "example. Here, we explain how a French sentence is input into the\n",
      "transformer and a corresponding English translation is output. We will\n",
      "also describe each of ...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "response = query_engine.query(\"What is a transformer\")\n",
    "pprint_response(response, show_source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.indices.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "retriever = VectorIndexRetriever(index=index, similarity_top_k=4)\n",
    "postprocess = SimilarityPostprocessor(similarity_cutoff=0.80)\n",
    "query_engine = RetrieverQueryEngine(retriever=retriever,\n",
    "                                    node_postprocessors=[postprocess])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: A transformer is a sequence-to-sequence model\n",
      "developed at Google in 2017 for tasks like translation. It consists of\n",
      "an encoder and a decoder, with the encoder converting input text into\n",
      "a representation that is then used by the decoder to generate output\n",
      "text. The transformer architecture includes multiple layers such as\n",
      "Multi-Head Attention, Add & Norm, Feed-Forward, Linear, and Softmax,\n",
      "with the hidden layers being where the main processing occurs.\n",
      "______________________________________________________________________\n",
      "Source Node 1/1\n",
      "Node ID: 484185cd-7b42-469d-a346-e9f0edcc59f8\n",
      "Similarity: 0.8106367030469969\n",
      "Text: Foundational Large Language Models & Text Generation 9 September\n",
      "2024 Transformer The transformer architecture was developed at Google\n",
      "in 2017 for use in a translation model.1  It’s a sequence-to-sequence\n",
      "model capable of converting sequences from one domain  into sequences\n",
      "in another domain. For example, translating French sentences to\n",
      "English ...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "response = query_engine.query(\"What is a transformer\")\n",
    "pprint_response(response, show_source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  96%|█████████▌| 23/24 [1:05:13<02:50, 170.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: A transformer is a sequence-to-sequence model that\n",
      "consists of an encoder and a decoder. The encoder converts input text\n",
      "into a representation, which is then passed to the decoder for\n",
      "generating the output text. It was developed at Google in 2017 for\n",
      "tasks like translation, where it can convert sequences from one domain\n",
      "into sequences in another domain.\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from llama_index.core import (VectorStoreIndex,\n",
    "                              SimpleDirectoryReader,\n",
    "                              StorageContext,\n",
    "                              load_index_from_storage)\n",
    "\n",
    "#Check if storage already exists\n",
    "PERSIST_DIR = \"./storage\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    #load documents and create index\n",
    "    documents = SimpleDirectoryReader('LLM documents).load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents=documents)\n",
    "    #store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    #load existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context=storage_context)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is a transformer ?\")\n",
    "pprint_response(response=response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Challenges in cybersecurity include horizontal and vertical scalability, availability, data consistency, real-time updates, backups, access control, compliance, and other considerations related to managing embeddings and vector stores. Additionally, managing mutations in embeddings over time, balancing the cost of frequently updating embeddings, and ensuring a well-defined automated process for storing, managing, and possibly purging embeddings from vector databases are also significant challenges in cybersecurity.'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:5601/query\"\n",
    "query = \"What are the challenges in cybersecurity\"\n",
    "\n",
    "response = requests.get(url, params={\"text\": query})\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'File uploaded and inserted into index!'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# File path to upload\n",
    "file_path = 'LLM.pdf'\n",
    "\n",
    "# Endpoint URL\n",
    "url = \"http://localhost:5601/uploadFile\"\n",
    "\n",
    "# Send the file\n",
    "with open(file_path, 'rb') as f:\n",
    "    files = {'file': f}\n",
    "    response = requests.post(url, files=files)\n",
    "\n",
    "# Print the response\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Different types of prompting techniques include prompt expansion, semantic search, business rules, and coherence checks.'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:5601/query\"\n",
    "query = \"What are different types of prompting techniques?\"\n",
    "\n",
    "response = requests.get(url, params={\"text\": query})\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
